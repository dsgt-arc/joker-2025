{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install panphon\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrUCKsH7poTz",
        "outputId": "7b3fc8a9-1e58-498c-edc6-9fb7cac57f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting panphon\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from panphon) (75.2.0)\n",
            "Collecting unicodecsv (from panphon)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from panphon) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from panphon) (2024.11.6)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.11/dist-packages (from panphon) (2.0.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from panphon) (0.8.1)\n",
            "Collecting munkres (from panphon)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Downloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10744 sha256=0d0a09415dff6ab98141fab4b083fae99ea16da407f9898d0e1c137679d72f06\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/03/6f/d2e0162d94c0d451556fa43dd4d5531457245c34a36b41ef4a\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, panphon\n",
            "Successfully installed munkres-1.1.4 panphon-0.21.2 unicodecsv-0.14.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import panphon\n",
        "import numpy as np\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "def ipa_to_feature_vector(ipa_string):\n",
        "    segments = ft.ipa_segs(ipa_string)\n",
        "\n",
        "    mapping = {'+': 1, '-': -1, '0': 0}\n",
        "\n",
        "    vectors = [\n",
        "        [mapping[feat] for feat in ft.segment_to_vector(seg)]\n",
        "        for seg in segments\n",
        "    ]\n",
        "\n",
        "    return np.array(vectors, dtype=float)\n",
        "\n",
        "ipa = 'ynivɛʁsalizəʁɔ̃'  # bonjour\n",
        "vec = ipa_to_feature_vector(ipa)\n",
        "\n",
        "print(f\"IPA: {ipa}\")\n",
        "print(f\"Feature vector: {vec}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4bLsw0Y3xcn",
        "outputId": "d0351a7d-4d42-4f52-ae13-d1f02bd59cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IPA: ynivɛʁsalizəʁɔ̃\n",
            "Feature vector: [[ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0.  1.  1. -1. -1.\n",
            "   1. -1.  1. -1.  0.  0.]\n",
            " [-1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1.  1. -1. -1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  0.  1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1. -1.\n",
            "  -1. -1. -1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1.  1.  1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1.  1. -1. -1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1. -1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1.  1.\n",
            "   1. -1. -1. -1.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import panphon\n",
        "import numpy as np\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "# (1) from the paper\n",
        "def phoneme_feature_set(segment):\n",
        "    features = ft.names\n",
        "    vector = ft.segment_to_vector(segment)\n",
        "    return {features[i] for i, val in enumerate(vector) if val == '+'}\n",
        "\n",
        "#(2)\n",
        "def bigram_feature_set(p1, p2):\n",
        "    return phoneme_feature_set_extended(p1).union(phoneme_feature_set_extended(p2))\n",
        "\n",
        "#(3) bigram similarity (jaccard similarity of bigram features)\n",
        "def bigram_similarity(bg1, bg2):\n",
        "    set1 = bigram_feature_set(*bg1)\n",
        "    set2 = bigram_feature_set(*bg2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union else 0\n",
        "\n",
        "def ipa_to_bigrams(ipa):\n",
        "    phonemes = ft.ipa_segs(ipa)\n",
        "    phonemes = ['BEG'] + phonemes + ['END']\n",
        "    return list(zip(phonemes[:-1], phonemes[1:]))\n",
        "\n",
        "def phoneme_feature_set_extended(segment):\n",
        "    if segment == 'BEG':\n",
        "        return {'beg'}\n",
        "    elif segment == 'END':\n",
        "        return {'end'}\n",
        "    else:\n",
        "        return phoneme_feature_set(segment)\n",
        "\n",
        "#(5) from the paper\n",
        "def word_similarity_bigrams(ipa1, ipa2):\n",
        "    seq1 = ipa_to_bigrams(ipa1)\n",
        "    seq2 = ipa_to_bigrams(ipa2)\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    dp = np.zeros((n+1, m+1))\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        dp[i, 0] = dp[i-1, 0]\n",
        "    for j in range(1, m+1):\n",
        "        dp[0, j] = dp[0, j-1]\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            sim = bigram_similarity(seq1[i-1], seq2[j-1])\n",
        "            dp[i, j] = max(\n",
        "                dp[i-1, j-1] + sim,  # match/substitution\n",
        "                dp[i-1, j],          # deletion\n",
        "                dp[i, j-1]           # insertion\n",
        "            )\n",
        "\n",
        "    # (6) from the paper normalize by the length of the longest sequence\n",
        "    max_len = max(n, m)\n",
        "    return dp[n, m] / max_len\n",
        "\n",
        "ipa1 = 'tɥa'\n",
        "ipa2 = 'tɥa'\n",
        "\n",
        "similarity_score = word_similarity_bigrams(ipa1, ipa2)\n",
        "print(f\"similarity: {similarity_score:.3f}\")\n",
        "print(ipa_to_bigrams(ipa1))\n",
        "print(ipa_to_bigrams(ipa2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-mPpp74CSr2",
        "outputId": "57be710a-7a94-46e0-8dfa-0d1e12d270f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity: 1.000\n",
            "[('BEG', 't'), ('t', 'ɥ'), ('ɥ', 'a'), ('a', 'END')]\n",
            "[('BEG', 't'), ('t', 'ɥ'), ('ɥ', 'a'), ('a', 'END')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/converted_phrases.csv\")\n",
        "words = df[\"word\"].tolist()\n",
        "ipas = df[\"ipa\"].tolist()\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}"
      ],
      "metadata": {
        "id": "oT0H088FKQx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "char_counter = Counter(\"\".join(ipas))\n",
        "phoneme_vocab = {ch: i + 1 for i, ch in enumerate(char_counter)}  # reserve 0 for PAD\n",
        "phoneme_vocab['<PAD>'] = 0\n",
        "vocab_size = len(phoneme_vocab)"
      ],
      "metadata": {
        "id": "cR0XTVK6jDyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ipa_to_ids(ipa, max_len=30):\n",
        "    ids = [phoneme_vocab.get(ch, 0) for ch in ipa]\n",
        "    return ids[:max_len] + [0] * (max_len - len(ids))\n",
        "\n",
        "ipa_ids = [ipa_to_ids(ipa) for ipa in ipas]"
      ],
      "metadata": {
        "id": "sE_2g3FOjIf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiLSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        out = out.mean(dim=1)  # mean pooling\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "JS_iHDJJKFKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "embed_dim = 32\n",
        "hidden_dim = 64\n",
        "output_dim = 50\n",
        "encoder = BiLSTMEncoder(vocab_size, embed_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for i in tqdm(range(0, len(ipa_ids), batch_size)):\n",
        "        batch_indices = list(range(i, min(i + batch_size, len(ipa_ids))))\n",
        "        batch_x = [ipa_ids[idx] for idx in batch_indices]\n",
        "        batch_y = np.random.choice(len(ipa_ids), len(batch_x), replace=False)\n",
        "\n",
        "        x_tensor = torch.tensor(batch_x)\n",
        "        y_tensor = torch.tensor([ipa_ids[j] for j in batch_y])\n",
        "\n",
        "        emb_x = encoder(x_tensor)\n",
        "        emb_y = encoder(y_tensor)\n",
        "\n",
        "        emb_x = F.normalize(emb_x, dim=1)\n",
        "        emb_y = F.normalize(emb_y, dim=1)\n",
        "\n",
        "        pred_sim = torch.sum(emb_x * emb_y, dim=1)\n",
        "        pred_sim = torch.clamp(pred_sim, 0, 1)\n",
        "\n",
        "        target_sim = torch.tensor([\n",
        "            word_similarity_bigrams(ipas[i + k], ipas[j])\n",
        "            for k, j in enumerate(batch_y)\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "        loss = loss_fn(pred_sim, target_sim)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwOuHpk2ie2K",
        "outputId": "e53db0df-d835-4eca-ef55-9207b22c5dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1922/1922 [13:12<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 25.3110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1922/1922 [13:01<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 11.3486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1922/1922 [13:03<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 7.4746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1922/1922 [12:59<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 5.8989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1922/1922 [13:05<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 5.0179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embedding_matrix = embeddings.weight.detach().numpy()\n",
        "\n",
        "# Normalize embeddings\n",
        "faiss.normalize_L2(embedding_matrix)\n",
        "\n",
        "# Build FAISS index for fast similarity search\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "index.add(embedding_matrix)\n",
        "\n",
        "# Retrieval function\n",
        "def retrieve_similar_words(query_word, top_k=10):\n",
        "    idx = word_to_idx[query_word]\n",
        "    query_emb = embedding_matrix[idx].reshape(1, -1)\n",
        "    faiss.normalize_L2(query_emb)\n",
        "\n",
        "    similarities, indices = index.search(query_emb, top_k+1)\n",
        "\n",
        "    # Exclude the word itself from the results\n",
        "    results = [(words[i], similarities[0][j])\n",
        "               for j, i in enumerate(indices[0]) if i != idx]\n",
        "\n",
        "    return results[:top_k]\n",
        "\n",
        "# Example retrieval:\n",
        "print(retrieve_similar_words('bonjour'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KOqH9Zrm8iZ",
        "outputId": "7fe108c1-4683-4091-9c42-dc22f936b986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('daigneras', np.float32(0.60518396)), ('dureriez', np.float32(0.54355437)), ('branches', np.float32(0.5393211)), ('cafte', np.float32(0.5342453)), ('aiguisait', np.float32(0.52750987)), ('redoutas', np.float32(0.5220129)), ('corsages', np.float32(0.5125512)), ('tantouze', np.float32(0.51016873)), ('ajustaient', np.float32(0.50593585)), ('grognassions', np.float32(0.5027506))]\n"
          ]
        }
      ]
    }
  ]
}