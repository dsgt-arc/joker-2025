{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4bLsw0Y3xcn",
        "outputId": "d0351a7d-4d42-4f52-ae13-d1f02bd59cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IPA: ynivɛʁsalizəʁɔ̃\n",
            "Feature vector: [[ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0.  1.  1. -1. -1.\n",
            "   1. -1.  1. -1.  0.  0.]\n",
            " [-1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1.  1. -1. -1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  0.  1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1. -1.\n",
            "  -1. -1. -1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1.  1.  1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1.  1. -1. -1.\n",
            "  -1. -1.  1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1. -1. -1.  0.  0.]\n",
            " [-1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  0. -1. -1. -1.  1.\n",
            "  -1. -1.  0. -1.  0.  0.]\n",
            " [ 1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  0. -1.  0. -1. -1. -1.  1.\n",
            "   1. -1. -1. -1.  0.  0.]]\n"
          ]
        }
      ],
      "source": [
        "import panphon\n",
        "import numpy as np\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "def ipa_to_feature_vector(ipa_string):\n",
        "    segments = ft.ipa_segs(ipa_string)\n",
        "\n",
        "    mapping = {'+': 1, '-': -1, '0': 0}\n",
        "\n",
        "    vectors = [\n",
        "        [mapping[feat] for feat in ft.segment_to_vector(seg)]\n",
        "        for seg in segments\n",
        "    ]\n",
        "\n",
        "    return np.array(vectors, dtype=float)\n",
        "\n",
        "ipa = 'ynivɛʁsalizəʁɔ̃'  # bonjour\n",
        "vec = ipa_to_feature_vector(ipa)\n",
        "\n",
        "print(f\"IPA: {ipa}\")\n",
        "print(f\"Feature vector: {vec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-mPpp74CSr2",
        "outputId": "57be710a-7a94-46e0-8dfa-0d1e12d270f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity: 1.000\n",
            "[('BEG', 't'), ('t', 'ɥ'), ('ɥ', 'a'), ('a', 'END')]\n",
            "[('BEG', 't'), ('t', 'ɥ'), ('ɥ', 'a'), ('a', 'END')]\n"
          ]
        }
      ],
      "source": [
        "import panphon\n",
        "import numpy as np\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "# (1) from the paper\n",
        "def phoneme_feature_set(segment):\n",
        "    features = ft.names\n",
        "    vector = ft.segment_to_vector(segment)\n",
        "    return {features[i] for i, val in enumerate(vector) if val == '+'}\n",
        "\n",
        "#(2)\n",
        "def bigram_feature_set(p1, p2):\n",
        "    return phoneme_feature_set_extended(p1).union(phoneme_feature_set_extended(p2))\n",
        "\n",
        "#(3) bigram similarity (jaccard similarity of bigram features)\n",
        "def bigram_similarity(bg1, bg2):\n",
        "    set1 = bigram_feature_set(*bg1)\n",
        "    set2 = bigram_feature_set(*bg2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union else 0\n",
        "\n",
        "def ipa_to_bigrams(ipa):\n",
        "    phonemes = ft.ipa_segs(ipa)\n",
        "    phonemes = ['BEG'] + phonemes + ['END']\n",
        "    return list(zip(phonemes[:-1], phonemes[1:]))\n",
        "\n",
        "def phoneme_feature_set_extended(segment):\n",
        "    if segment == 'BEG':\n",
        "        return {'beg'}\n",
        "    elif segment == 'END':\n",
        "        return {'end'}\n",
        "    else:\n",
        "        return phoneme_feature_set(segment)\n",
        "\n",
        "#(5) from the paper\n",
        "def word_similarity_bigrams(ipa1, ipa2):\n",
        "    seq1 = ipa_to_bigrams(ipa1)\n",
        "    seq2 = ipa_to_bigrams(ipa2)\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    dp = np.zeros((n+1, m+1))\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        dp[i, 0] = dp[i-1, 0]\n",
        "    for j in range(1, m+1):\n",
        "        dp[0, j] = dp[0, j-1]\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            sim = bigram_similarity(seq1[i-1], seq2[j-1])\n",
        "            dp[i, j] = max(\n",
        "                dp[i-1, j-1] + sim,  # match/substitution\n",
        "                dp[i-1, j],          # deletion\n",
        "                dp[i, j-1]           # insertion\n",
        "            )\n",
        "\n",
        "    # (6) from the paper normalize by the length of the longest sequence\n",
        "    max_len = max(n, m)\n",
        "    return dp[n, m] / max_len\n",
        "\n",
        "ipa1 = 'tɥa'\n",
        "ipa2 = 'tɥa'\n",
        "\n",
        "similarity_score = word_similarity_bigrams(ipa1, ipa2)\n",
        "print(f\"similarity: {similarity_score:.3f}\")\n",
        "print(ipa_to_bigrams(ipa1))\n",
        "print(ipa_to_bigrams(ipa2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oT0H088FKQx-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"converted_phrases.csv\")\n",
        "words = df[\"word\"].tolist()\n",
        "ipas = df[\"ipa\"].tolist()\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cR0XTVK6jDyD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "char_counter = Counter(\"\".join(ipas))\n",
        "phoneme_vocab = {ch: i + 1 for i, ch in enumerate(char_counter)}  # reserve 0 for PAD\n",
        "phoneme_vocab['<PAD>'] = 0\n",
        "vocab_size = len(phoneme_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sE_2g3FOjIf9"
      },
      "outputs": [],
      "source": [
        "def ipa_to_ids(ipa, max_len=30):\n",
        "    ids = [phoneme_vocab.get(ch, 0) for ch in ipa]\n",
        "    return ids[:max_len] + [0] * (max_len - len(ids))\n",
        "\n",
        "ipa_ids = [ipa_to_ids(ipa) for ipa in ipas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JS_iHDJJKFKU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiLSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        out = out.mean(dim=1)  # mean pooling\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwOuHpk2ie2K",
        "outputId": "e53db0df-d835-4eca-ef55-9207b22c5dc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:57<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 25.1928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:41<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 11.3533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:58<00:00,  1.28it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 7.3932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:42<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Loss: 5.9039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:38<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 5.3056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:34<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Loss: 4.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:35<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Loss: 4.4024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:42<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Loss: 4.2182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:35<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Loss: 3.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1922/1922 [24:35<00:00,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 3.8504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "embed_dim = 32\n",
        "hidden_dim = 64\n",
        "output_dim = 50\n",
        "encoder = BiLSTMEncoder(vocab_size, embed_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(10):\n",

        "    total_loss = 0\n",
        "    for i in tqdm(range(0, len(ipa_ids), batch_size)):\n",
        "        batch_indices = list(range(i, min(i + batch_size, len(ipa_ids))))\n",
        "        batch_x = [ipa_ids[idx] for idx in batch_indices]\n",
        "        batch_y = np.random.choice(len(ipa_ids), len(batch_x), replace=False)\n",
        "\n",
        "        x_tensor = torch.tensor(batch_x)\n",
        "        y_tensor = torch.tensor([ipa_ids[j] for j in batch_y])\n",
        "\n",
        "        emb_x = encoder(x_tensor)\n",
        "        emb_y = encoder(y_tensor)\n",
        "\n",
        "        emb_x = F.normalize(emb_x, dim=1)\n",
        "        emb_y = F.normalize(emb_y, dim=1)\n",
        "\n",
        "        pred_sim = torch.sum(emb_x * emb_y, dim=1)\n",
        "        pred_sim = torch.clamp(pred_sim, 0, 1)\n",
        "\n",
        "        target_sim = torch.tensor([\n",
        "            word_similarity_bigrams(ipas[i + k], ipas[j])\n",
        "            for k, j in enumerate(batch_y)\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "        loss = loss_fn(pred_sim, target_sim)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,

      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KOqH9Zrm8iZ",
        "outputId": "7fe108c1-4683-4091-9c42-dc22f936b986"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding IPA embeddings:   0%|          | 0/1922 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding IPA embeddings: 100%|██████████| 1922/1922 [00:17<00:00, 106.89it/s]\n"
          ]
        }
      ],
      "source": [
        "def encode_all_words_batched(ipa_ids, batch_size=128):\n",
        "    encoder.eval()\n",
        "    all_embeds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(ipa_ids), batch_size), desc=\"Encoding IPA embeddings\"):\n",
        "            batch = ipa_ids[i:i+batch_size]\n",
        "            batch_tensor = torch.tensor(batch)\n",
        "            emb = encoder(batch_tensor)\n",
        "            emb = F.normalize(emb, dim=1)\n",
        "            all_embeds.append(emb.cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeds)\n",
        "\n",
        "embedding_matrix = encode_all_words_batched(ipa_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "faiss.normalize_L2(embedding_matrix)\n",
        "index = faiss.IndexFlatIP(embedding_matrix.shape[1])\n",
        "index.add(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_to_idx = {word: i for i, word in enumerate(words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_similar_words(query_word, top_k=10):\n",
        "    if query_word not in word_to_idx:\n",
        "        raise ValueError(f\"'{query_word}' not found in vocabulary.\")\n",
        "\n",
        "    query_idx = word_to_idx[query_word]\n",
        "    query_emb = embedding_matrix[query_idx].reshape(1, -1)\n",
        "\n",
        "    faiss.normalize_L2(query_emb)  \n",
        "\n",
        "    similarities, indices = index.search(query_emb, top_k + 1)\n",
        "\n",
        "    results = []\n",
        "    for j, i in enumerate(indices[0]):\n",
        "        if i != query_idx: \n",
        "            results.append((words[i], similarities[0][j]))\n",
        "\n",
        "    return results[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  bonjours (score: 1.000)\n",
            "  bûcheur (score: 0.918)\n",
            "  bûcheurs (score: 0.918)\n",
            "  bougèrent (score: 0.915)\n",
            "  bouchèrent (score: 0.909)\n",
            "  bouchères (score: 0.909)\n",
            "  bouchère (score: 0.909)\n",
            "  brochures (score: 0.902)\n",
            "  brochure (score: 0.902)\n",
            "  bougeoirs (score: 0.897)\n"
          ]
        }
      ],
      "source": [
        "results = retrieve_similar_words(\"bonjour\", top_k=10)\n",
        "for word, score in results:\n",
        "    print(f\"  {word} (score: {score:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(encoder.state_dict(), \"bilstm_encoder.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"embedding_matrix.npy\", embedding_matrix)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
