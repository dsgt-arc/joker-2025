{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20240620 \n",
      " Comment les astronautes ajoutent-ils plus de protéines à leur alimentation ? Ils la rendent météorique. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "max_tokens = 1024\n",
    "translation_instructions = \"\"\"\n",
    "                    Translate the pun to French. Only include the translated pun in your response. \n",
    "\"\"\"\n",
    "\n",
    "pun =\"Input: How do spacemen add more protein to their diet ? They make it meteor .\"\n",
    "\n",
    "input = translation_instructions + pun\n",
    "\n",
    "\n",
    "model = 'claude-3-5-sonnet-20240620'\n",
    "api_key = \"\"\n",
    "llm = ChatAnthropic(model=model, api_key=api_key, max_tokens=max_tokens, temperature=1)\n",
    "translation_response = llm.invoke(input)\n",
    "print(model, '\\n', translation_response.content, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20240620 \n",
      " I would rate this translation as Part Equivalence. Here's why:\n",
      "\n",
      "1. Contextual meaning: The overall joke structure and context are preserved in the French translation. Both versions play on the idea of astronauts and space-related terms to create a food-related pun.\n",
      "\n",
      "2. Wordplay: \n",
      "   - English: \"meteor\" sounds like \"meatier\" (more meaty/protein-rich)\n",
      "   - French: \"météorique\" (meteoric) is used as an adjective to describe the food\n",
      "\n",
      "3. Humor: The French version maintains a level of humor, although it's slightly different from the original.\n",
      "\n",
      "4. Literal meaning: The literal meanings differ somewhat. The English version suggests \"making\" the food meteor-like, while the French version describes \"rendering\" the food meteoric.\n",
      "\n",
      "5. Intended effect: Both versions aim to create a play on words related to space and food, but the exact mechanism of the wordplay differs.\n",
      "\n",
      "The translation succeeds in maintaining the space theme and the general humorous intent. However, the specific wordplay of \"meteor/meatier\" is lost in translation. The French version opts for a more straightforward use of the space-related term \"météorique,\" which doesn't have the same double meaning as the English \"meteor/meatier\" play.\n",
      "\n",
      "While the joke still works in French, it loses some of the clever phonetic play of the original. This shift in the specific mechanism of the pun, while maintaining the overall theme and humor, is why I would classify this as Part Equivalence rather than Full Equivalence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "equivalence_evaluator_instructions = \"\"\"\n",
    "\n",
    "You are fluent in both English and French, with a deep understanding of humor in both languages. Your task is to compare a translated pun to its original version and \n",
    "determine whether the meanings of the source is maintained in the translation, and if the translation is still humorous.\n",
    "\n",
    "Rate the translation using one of the following categories:\n",
    "\n",
    "    Full Equivalence: Both the literal and contextual meanings of the pun remain the same in the translation. The humor, wordplay, and intended effect are fully preserved.\n",
    "    Part Equivalence: The contextual meaning of the pun is similar in both languages, but the literal meaning of the target word differs. While the translation remains \n",
    "        metaphorical, the wordplay may be altered.\n",
    "    Non-Equivalence: The contextual meaning is somewhat preserved, but the translation is no longer metaphorical. The literal meaning of the target words differs significantly, \n",
    "        resulting in a loss of the original wordplay.\n",
    "\n",
    "Evaluate the pun based on these criteria and provide a justification for your rating.\n",
    "                    \n",
    "\"\"\"\n",
    "\n",
    "input = equivalence_evaluator_instructions + '\\n' + pun + '\\n' +  translation_response.content\n",
    "equivalence_response = llm.invoke(input)\n",
    "print(model, '\\n', equivalence_response.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20240620 \n",
      " Rating: Misunderstanding\n",
      "\n",
      "Justification:\n",
      "The French translation of this pun attempts to maintain the literal meaning of the wordplay, but fails to convey the intended humor effectively. \n",
      "\n",
      "In the English version, the pun relies on the homophonic nature of \"meteor\" and \"meet-er\" (as in \"to make something meet a certain standard\"). The joke plays on the space theme with \"meteor\" while also implying that astronauts are increasing (\"meeting\") their protein intake.\n",
      "\n",
      "The French translation \"météorique\" does preserve the space-related theme and the literal meaning of \"meteor.\" However, it loses the dual meaning present in the English version. In French, \"météorique\" doesn't have a secondary meaning related to increasing or meeting a standard. \n",
      "\n",
      "While the translation maintains the space theme and the literal reference to meteors, it doesn't capture the wordplay that makes the original pun humorous. A French speaker would understand the space reference but would likely miss the intended joke about increasing protein intake.\n",
      "\n",
      "This results in a situation where the literal elements of the pun are preserved, but the contextual meaning and the intended humor are lost in translation, fitting the criteria for a \"Misunderstanding\" rating. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mistranslation_evaluator_instructions = \"\"\"\n",
    "\n",
    "You are fluent in both English and French, with a deep understanding of humor in both languages. Your task is to compare a translated pun to its original version and \n",
    "assess whether the literal meaning of the pun's wordplay is similar in both languages, but the contextual meaning or intended humor is lost or altered in translation.\n",
    "\n",
    "Rate the translation using one of the following categories:\n",
    "    Misunderstanding: The literal meaning of the pun’s wordplay is similar in both the source text and the translation,\n",
    "        but the translation fails to convey the contextual meaning or intended humor of the original pun.\n",
    "    Error: The pun’s wordplay is mistranslated, meaning that both the literal and contextual meanings differ between the source and translation, \n",
    "        resulting in a complete loss of the intended pun or humor.\n",
    "\n",
    "Evaluate the pun based on these criteria and provide a justification for your rating.\n",
    "\"\"\"\n",
    "\n",
    "input = mistranslation_evaluator_instructions  + '\\n' + pun  +  '\\n' + translation_response.content\n",
    "mistranslation_response = llm.invoke(input)\n",
    "print(model, '\\n', mistranslation_response.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20240620 \n",
      " Rating: Less\n",
      "\n",
      "Justification:\n",
      "\n",
      "The original English pun plays on the word \"meteor\" which sounds similar to \"meatier\" (containing more meat, thus more protein). This wordplay creates a clever connection between space-related terminology and the concept of adding protein to one's diet.\n",
      "\n",
      "The French translation attempts to maintain the space theme with \"météorique\" (meteoric), but loses the direct connection to protein or meat. In French, \"météorique\" is typically used figuratively to describe something that happens suddenly or rapidly, like a meteoric rise to fame.\n",
      "\n",
      "While the French version still maintains a space-related punchline, it doesn't carry the same level of wordplay or connection to the setup about protein. The loss of this direct link reduces the cleverness and satisfaction that comes from \"getting\" the pun, which in turn results in less emotional impact.\n",
      "\n",
      "The English version elicits a stronger emotional response - likely a groan or chuckle - due to the unexpected connection between \"meteor\" and \"meatier.\" The French version, while still maintaining the space theme, doesn't provide the same level of surprise or cleverness, resulting in a reduced emotional reaction.\n",
      "\n",
      "Therefore, the French translation conveys less emotion compared to the original English pun. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "emotion_evaluator_instructions = \"\"\"\n",
    "\n",
    "You are fluent in both English and French, with a deep understanding of humor in both languages. Your task is to compare a translated pun to its original version and \n",
    "assess to what extent the original pun's wordplay and its translation convey different amounts of emotion.\n",
    "\n",
    "Rate the emotion of the translation compared to the original using one of the following categories:\n",
    "    Zero\n",
    "    Less\n",
    "    Same\n",
    "    More\n",
    "\n",
    "Evaluate the pun based on these criteria and provide a justification for your rating.\n",
    "\"\"\"\n",
    "input = emotion_evaluator_instructions + '\\n' + pun + '\\n' +  translation_response.content\n",
    "emotion_response = llm.invoke(input)\n",
    "print(model, '\\n', emotion_response.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20240620 \n",
      " Rating: Somewhat Likely\n",
      "\n",
      "Justification:\n",
      "\n",
      "The French translation of this pun attempts to maintain the wordplay present in the original English version, but it doesn't quite achieve the same level of humor or natural flow. Here's a breakdown:\n",
      "\n",
      "1. The setup of the joke is translated accurately and naturally: \"Comment les astronautes ajoutent-ils plus de protéines à leur alimentation ?\" This part would be easily understood by a native French speaker.\n",
      "\n",
      "2. The punchline, \"Ils la rendent météorique,\" is where the translation becomes less effective:\n",
      "\n",
      "   - The word \"météorique\" does exist in French, but it's typically used figuratively to describe something that happens very quickly or spectacularly, rather than literally referring to meteors.\n",
      "   - The connection between \"météorique\" and protein intake is not as clear or intuitive as the English \"meteor/meet her\" pun.\n",
      "   - The use of \"la rendent\" (make it) is grammatically correct but doesn't flow as naturally in this context as the English version.\n",
      "\n",
      "3. A native French speaker would likely understand the attempt at wordplay, recognizing the space theme and the connection to \"météore\" (meteor). However, the pun doesn't land as smoothly as the English version.\n",
      "\n",
      "4. The translation maintains the space theme and attempts to create a play on words, which is commendable, but it lacks the natural flow and immediate comprehension that a truly successful pun would have.\n",
      "\n",
      "Given these factors, a native French speaker would likely recognize this as an attempt at a pun, but it might not elicit the same level of amusement or appreciation as the original English version. The translation is understandable but doesn't quite capture the effortless wordplay of the original, hence the \"Somewhat Likely\" rating. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "authenticity_evaluator_instructions = \"\"\"\n",
    "\n",
    "You are fluent in both English and French, with a deep understanding of humor in both languages. Your task is to compare a translated pun to its original version and \n",
    "assess to what extent the translated pun reads like standard, well-edited language, such that the pun would be understood by a native speaker of the French language. \n",
    "Rate the translation using one of the following categories:\n",
    "    Not at all likely\n",
    "    Not Very Likely\n",
    "    Somewhat Likely\n",
    "    Very Likely\n",
    "    Extremely Likely\n",
    "\n",
    "Evaluate the pun based on these criteria and provide a justification for your rating.\n",
    "\"\"\"\n",
    "input = authenticity_evaluator_instructions  + '\\n' + pun + '\\n' +  translation_response.content\n",
    "authenticity_response = llm.invoke(input)\n",
    "print(model, '\\n', authenticity_response.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
